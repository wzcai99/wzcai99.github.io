<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Wenzhe Cai</title>
    <meta name="author" content="Wenzhe Cai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/icon.jpeg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr onmouseout="pixnav_stop()" onmouseover="pixnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='pixnav_image'>
            <img src='images/PixNav.jpg' width="180"></div>
            <img src='images/PixNav.jpg' width="170">
          </div>
          <script type="text/javascript">
            function pixnav_start() {
              document.getElementById('pixnav_image').style.opacity = "1";
            }
            function pixnav_stop() {
              document.getElementById('pixnav_image').style.opacity = "0";
            }
            pixnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.10309.pdf">
            <span class="papertitle">Bridging Zero-Shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</span>
          </a>
          <br>
          <strong>Wenzhe Cai</strong>, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong
          <br>
          <em>Arxiv 2023</em>
          <br>
          <a href="https://sites.google.com/view/pixnav">Website</a>
          /
          <a href="https://arxiv.org/pdf/2309.10309.pdf">Paper</a>
	        /
          <a href="https://github.com/wzcai99/Pixel-Navigator">Github</a>
          <p>
          An RGB-based solution (without explicit map) for zero-shot object navigation.
          </p>
        </td>
      </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests include Embodied AI, Visual Navigation and Deep Reinforcement Learning.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='alignerf_image'>
                  <img src='images/PixNav.jpg' width="160"></div>
                <img src='images/PixNav.jpg' width="160">
              </div>
              <script type="text/javascript">
                function alignerf_start() {
                  document.getElementById('alignerf_image').style.opacity = "1";
                }

                function alignerf_stop() {
                  document.getElementById('alignerf_image').style.opacity = "0";
                }
                alignerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Bridging Zero-Shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</span>
              <br>
              <strong> Wenzhe Cai</strong>, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong
              <br>
              <em>Arxiv, 2023</em>
              <br>
              <a href="https://sites.google.com/view/pixnav/">website</a>
              /
              <a href="https://arxiv.org/pdf/2309.10309">paper</a>
              /
              <a href="https://github.com/wzcai99/Pixel-Navigator">github</a>
              <p></p>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='amesac_image'>
                <img src='images/AME-SAC.png' width="180"></div>
                <img src='images/AME-SAC.png' width="180">
              </div>
              <script type="text/javascript">
                function amesac_start() {
                  document.getElementById('amesac_image').style.opacity = "1";
                }
                function amesac_stop() {
                  document.getElementById('amesac_image').style.opacity = "0";
                }
                amesac_stop()
              </script>
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Robust Navigation with Cross-Modal Fusion and Knowledge Transfer</span>
                <br>
                <strong> Wenzhe Cai*</strong>, Guangran Cheng*, Lingyue Kong, Lu Dong, Changyin Sun
                <br>
                <em>IEEE Conference on Robotics and Automation (ICRA), 2023</em>
                <br>
                <a href="https://sites.google.com/view/distillnav/">website</a>
                /
                <a href="https://arxiv.org/abs/2309.13266">paper</a>
                /
                <a href="https://github.com/wzcai99/Distill-Navigator">github</a>
                <p></p>
              </td>
            </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='amesac_image'>
                  <img src='images/AME-SAC.png' width="180"></div>
                  <img src='images/AME-SAC.png' width="180">
                </div>
                <script type="text/javascript">
                  function amesac_start() {
                    document.getElementById('amesac_image').style.opacity = "1";
                  }
                  function amesac_stop() {
                    document.getElementById('amesac_image').style.opacity = "0";
                  }
                  amesac_stop()
                </script>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Multi-Task Reinforcement Learning With Attention-Based Mixture of Experts</span>
                  <br>
                  Guangran Cheng, Lu Dong, <Strong>Wenzhe Cai</Strong>, Changyin Sun
                  <br>
                  <em>IEEE Robotics and Automation Letters (RA-L), 2023</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10111062">paper</a>
                  /
                  <a href="https://github.com/123penny123/AMESAC">github</a>
                  <p></p>
                </td>
              </tr>
              </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='worldmodel_image'>
                  <img src='images/WorldModel.png' width="180"></div>
                  <img src='images/WorldModel.png' width="180">
                </div>
                <script type="text/javascript">
                  function discussnav_start() {
                    document.getElementById('worldmodel_image').style.opacity = "1";
                  }
                  function discussnav_stop() {
                    document.getElementById('worldmodel_image').style.opacity = "0";
                  }
                  discussnav_stop()
                </script>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Learning a World Model With Multitimescale Memory Augmentation</span>
                  <br>
                  <strong> Wenzhe Cai</strong>, Teng Wang, Jiawei Wang, Changyin Sun
                  <br>
                  <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2022</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9729537">paper</a>
                  <p></p>
                </td>
              </tr>
            </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Services</h2>
                <p> Reviewer: TNNLS, ICRA 2024.</p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
