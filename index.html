<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenzhe Cai</title>

    <meta name="author" content="Wenzhe Cai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/icon.jpeg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wenzhe Cai | 蔡文哲
                </p>
                <p> I am a researcher at the <a href=https://www.shlab.org.cn/>Shanghai AI Laboratory</a>, working closely with <a href=https://tai-wang.github.io/>Dr. Tai Wang</a> and <a href=https://oceanpang.github.io/>Dr. Jiangmiao Pang</a>.
                My research interests focus on Embodied AI, especially on building intelligent robots that can comprehend diverse language instructions and exhibit adaptive navigation behaviors in the dynamic open world.
                I obtained my Ph.D degree from <a href="https://www.seu.edu.cn/">Southeast University</a> advised by Prof. <a href="https://ieeexplore.ieee.org/author/37279060100">Changyin Sun</a>.  
                During my Ph.D period, I am fortunate to be a visiting student at <a href="https://english.pku.edu.cn/">Peking University</a>, advised by Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>.  
                </p>
                <p style="text-align:center">
                  <a href="mailto:wz_cai@seu.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=NHQcCyAAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/wzcai99/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:75%;max-width:75%">
                <a href="images/favicon/cwz.png"><img style="width:65%;max-width:65%" alt="profile photo" src="images/favicon/cwz.png" class="hoverZoomLink"></a>
                <br>
                <table style="width:100%;"><tr><td></td></tr></table>
                <!-- <a href="https://badges.toozhao.com/stats/01HCRX73SF490PVTAZCMEX6V23"><img src="https://badges.toozhao.com/badges/01HCRX73SF490PVTAZCMEX6V23/blue.svg" /></a> -->
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests include Embodied AI, Visual Navigation and Deep Reinforcement Learning.
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/navdp.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance</span>
                <br>
                <strong>Wenzhe Cai</strong>, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, Jiangmiao Pang
                <br>
                <em> Arxiv, 2025 </em>
                <br>
                <a href="https://wzcai99.github.io/navigation-diffusion-policy.github.io/">website</a>
                /
                <a href="http://arxiv.org/abs/2505.08712">paper</a>
                /
                <a href="https://www.youtube.com/watch?v=vfUnxD9WfoA">video</a>
                /
                <a href="https://github.com/wzcai99/NavDP">github</a>
                <p>
                  We present a sim-to-real navigation diffusion policy that can achieve cross-embodiment generalization in dynamic, cluttered and diverse real-world scenarios.
                </p>
              </td>
            </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/streamvln.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling</span>
                  <br>
                  Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, <strong>Wenzhe Cai</strong>, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang
                  <br>
                  <em> Arxiv, 2025 </em>
                  <br>
                  <a href="https://streamvln.github.io/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2507.05240">paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=gG3mpefOBjc">video</a>
                  /
                  <a href="https://github.com/InternRobotics/StreamVLN">github</a>
                  <p>
                    StreamVLN is a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs.
                  </p>
                </td>
              </tr>
              </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/ImagineNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination</span>
                <br>
                Xinxin Zhao*, <strong>Wenzhe Cai*</strong>, Likun Tang, Teng Wang
                <br>
                <em> International Conference on Learning Representations (ICLR), 2025 </em>
                <br>
                <a href="https://wzcai99.github.io/">website</a>
                /
                <a href="https://arxiv.org/abs/2410.09874">paper</a>
                /
                <a href="https://wzcai99.github.io/">github</a>
                <p>
                  We propose a novel navigation decision framework, which first use imagination to generate candidate future images and let the VLMs to select.
                  This breaks ObjectGoal Navigation problem into a PointGoal navigation problem.            
                </p>
              </td>
            </tr>
            </tbody></table>
          
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/DILLM-VLN.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Boosting Efficient Reinforcement Learning for Vision-and-Language Navigation with Open-Sourced LLM</span>
                  <br>
                  Jiawei Wang, Teng Wang, <strong>Wenzhe Cai</strong>, Lele Xu, Changyin Sun
                  <br>
                  <em> IEEE Robotics and Automation Letters (RA-L), 2024 </em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10777561">paper</a>
                  /
                  <a href="https://github.com/wangjw55/DILLM">github</a>
                  <p>
                    We propose a hierarchical reinforcement learning method for vision-language navigation, which uses efficient open-sourced LLMs as a high-level planner and an RL-based policy for sub-instruction accomplishment.         
                  </p>
                </td>
              </tr>
              </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/InstructNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment</span>
                <br>
                Yuxing Long*, <strong>Wenzhe Cai*</strong>, Hongcheng Wang, Guanqi Zhan, Hao Dong
                <br>
                <em>Conference on Robot Learning (CoRL), 2024</em>
                <br>
                <a href="https://sites.google.com/view/instructnav/">website</a>
                /
                <a href="https://arxiv.org/html/2406.04882v1">paper</a>
                /
                <a href="https://github.com/LYX0501/InstructNav">github</a>
                <p>
                  We propose a zero-shot navigation system, InstructNav, which makes the first
                  endeavor to handle multi-task navigation problems without any navigation
                  training or pre-built maps. 
                </p>
              </td>
            </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/moddn.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation</span>
                  <br>
                  Hongcheng Wang, Peiqi Liu, <strong>Wenzhe Cai</strong>, Mingdong Wu, Zhengyu Qian, Hao Dong
                  <br>
                  <em> Conference on Neural Information Processing Systems (NeurIPS), 2024</em>
                  <br>
                  <a href="https://sites.google.com/view/moddn">website</a>
                  /
                  <a href="https://arxiv.org/abs/2410.03488">paper</a>
                  /
                  <a href="https://github.com/whcpumpkin/MO-DDN">github</a>
                  <p>
                    We propose the Multi-Object Demand-Driven Navigation (MO-DDN) benchmark, which evaluates the agent navigation performance in multi-object exploration and aligns with personalized demands.
                  </p>
                </td>
              </tr>
              </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                  <img src='images/PixNav.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Bridging Zero-Shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</span>
              <br>
              <strong> Wenzhe Cai</strong>, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong
              <br>
              <em>IEEE Conference on Robotics and Automation (ICRA), 2024</em>
              <br>
              <a href="https://sites.google.com/view/pixnav/">website</a>
              /
              <a href="https://arxiv.org/pdf/2309.10309">paper</a>
              /
              <a href="https://github.com/wzcai99/Pixel-Navigator">github</a>
              <p>
                We propose a pure RGB-based navigation skill, PixNav, which takes in an assigned pixel as goal specification and can be used to navigate towards any objects.
              </p>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/DiscussNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</span>
                <br>
                Yuxing Long, Xiaoqi Li, <strong>Wenzhe Cai</strong>, Hao Dong
                <br>
                <em>IEEE Conference on Robotics and Automation (ICRA), 2024</em>
                <br>
                <a href="https://sites.google.com/view/discussnav">website</a>
                /
                <a href="https://arxiv.org/pdf/2309.11382.pdf">paper</a>
                /
                <a href="https://github.com/LYX0501/DiscussNav">github</a>
                <p>
                  DiscussNav agent actively discusses with multiple domain experts before moving. And with multi-expert discussion, our method achieves zero-shot visual language navigation without any training.  
                </p>
              </td>
            </tr>
            </tbody></table>
          
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/AME-SAC.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Multi-Task Reinforcement Learning With Attention-Based Mixture of Experts</span>
                  <br>
                  Guangran Cheng, Lu Dong, <strong>Wenzhe Cai</strong>, Changyin Sun
                  <br>
                  <em>IEEE Robotics and Automation Letters (RA-L), 2023</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10111062">paper</a>
                  /
                  <a href="https://github.com/123penny123/AMESAC">github</a>
                  <p>
                  We propose a soft mixture of experts (MoE) based reinforcement learning method to tackle multi-task robotics control problems, which effectively captured the latent relationships among different tasks.
                  </p>
                </td>
              </tr>
              </tbody></table>

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/XuanPolicy.png' width="180">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <span class="papertitle">XuanCE: A Comprehensive and Unified Deep Reinforcement Learning Library</span>
                    <br>
                    Wenzhang Liu, <strong>Wenzhe Cai</strong>, Kun Jiang, Guangran Cheng, Yuanda Wang, Jiawei Wang, Jingyu Cao, Lele Xu, Chaoxu Mu, Changyin Sun
                    <br>
                    <em>Journel of Machine Learning Research (JMLR), 2023 (under review)</em>
                    <br>
                    <a href="https://github.com/agi-brain/xuanpolicy">github</a>
                    /
                    <a href="https://arxiv.org/pdf/2312.16248.pdf">paper</a>
                    <p>
                    XuanCE is an open-source ensemble of Deep Reinforcement Learning (DRL) algorithm implementations, which supports both single-agent RL and multi-agents RL algorithms.
                    </p>
                  </td>
                </tr>
                </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/DistillNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Robust Navigation with Cross-Modal Fusion and Knowledge Transfer</span>
                <br>
                <strong> Wenzhe Cai*</strong>, Guangran Cheng*, Lingyue Kong, Lu Dong, Changyin Sun 
                <br>
                <em>IEEE Conference on Robotics and Automation (ICRA), 2023</em>
                <br>
                <a href="https://sites.google.com/view/distillnav/">website</a>
                /
                <a href="https://arxiv.org/abs/2309.13266">paper</a>
                /
                <a href="https://github.com/wzcai99/Distill-Navigator">github</a>
                <p>
                  We propose a efficient distillation architecture to tackle the sim-to-real gap of an RL-based navigation policy. Our experiment shows our architecture outperforms the domain randomization techniques.
                </p>
              </td>
            </tr>
            </tbody></table>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/DGMem.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory</span>
                  <br>
                  <strong> Wenzhe Cai</strong>, Teng Wang, Guangran Cheng, Lele Xu, Changyin Sun 
                  <br>
                  <em>Applied Intelligence, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2311.18473">paper</a>
                  <p>
                    We discuss the self-supervised navigation problem and present Dynamic Graph Memory (DGMem), which facilitates training only with on-board observations. 
                  </p>
                </td>
              </tr>
              </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/WorldModel.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Learning a World Model with Multi-Timescale Memory Augmentation</span>
                  <br>
                  <strong> Wenzhe Cai</strong>, Teng Wang, Jiawei Wang, Changyin Sun 
                  <br>
                  <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2022</em>
                  <br>
                  <a href="https://arxiv.org/abs/2309.13266">paper</a>
                  <p>
                  We propose a novel action-conditioned video prediction method which introduces optical flow prediction to model the influence of actions and incorporate the optical-flow based image prediction to improve the long-term prediction quality.
                  </p>
                </td>
              </tr>
              </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Services</h2>
                <p> Reviewer: RAL, TNNLS, TAI, ICRA, ICLR, ICCV, CoRL, IROS.</p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Honers & Awards</h2>
                <p> SEU Doctoral Entrance Scholarship <br>
                    SEU Scholarship from Shanghai Zhang Jiang Hi-Tech Park <br>
                    SEU Scholarship from Jiangsu Zhongnan Construction Group <br>
                    SEU Merit Student <br>
                    First Prize in Robocup Rescue Simulation Competition, China (1st) <br>
                    First Prize in Electronic Design Competition, JiangSu <br>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
