<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenzhe Cai</title>

    <meta name="author" content="Wenzhe Cai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/icon.jpeg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wenzhe Cai | 蔡文哲
                </p>
                <p>I have been a PhD student since 2019 in the <a href="https://automation.seu.edu.cn/">School of Automation</a>, <a href="https://www.seu.edu.cn/">Southeast University</a>, advised by Prof. <a href="https://ieeexplore.ieee.org/author/37279060100">Changyin Sun</a>.
                Prior to this, I also got my bechlor degree in the school of Automation, Southeast University. 
                Currently, I am a research intern working on Embodied AI and visual navigation at <a href = "https://www.shlab.org.cn/" > Shanghai AI Laboratory </a>, advised by <a href = "https://oceanpang.github.io/"> Dr. Jiangmiao Pang </a>.
                Before that, I am fortunate to be a visiting student at <a href="https://english.pku.edu.cn/">Peking University</a>, advised by Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>.  
                I also used to be an intern at Noah's Ark Lab and Pengcheng Laboratory. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:wz_cai@seu.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=NHQcCyAAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/wzcai99/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:75%;max-width:75%">
                <a href="images/favicon/cwz.png"><img style="width:65%;max-width:65%" alt="profile photo" src="images/favicon/cwz.png" class="hoverZoomLink"></a>
                <br>
                <table style="width:100%;"><tr><td></td></tr></table>
                <a href="https://badges.toozhao.com/stats/01HCRX73SF490PVTAZCMEX6V23"><img src="https://badges.toozhao.com/badges/01HCRX73SF490PVTAZCMEX6V23/blue.svg" /></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests include Embodied AI, Visual Navigation and Deep Reinforcement Learning.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/InstructNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment</span>
                <br>
                Yuxing Long*, <strong>Wenzhe Cai*</strong>, Hongcheng Wang, Guanqi Zhan, Hao Dong
                <br>
                <em>Conference on Robot Learning (CoRL), 2024</em>
                <br>
                <a href="https://sites.google.com/view/instructnav/">website</a>
                /
                <a href="https://arxiv.org/html/2406.04882v1">paper</a>
                
                <!-- <a href="https://github.com/wzcai99/Pixel-Navigator">github</a> -->
                <p>
                  We propose a zero-shot navigation system, InstructNav, which makes the first
                  endeavor to handle multi-task navigation problems without any navigation
                  training or pre-built maps. 
                </p>
              </td>
            </tr>
            </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                  <img src='images/PixNav.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Bridging Zero-Shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</span>
              <br>
              <strong> Wenzhe Cai</strong>, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong
              <br>
              <em>IEEE Conference on Robotics and Automation (ICRA), 2024</em>
              <br>
              <a href="https://sites.google.com/view/pixnav/">website</a>
              /
              <a href="https://arxiv.org/pdf/2309.10309">paper</a>
              /
              <a href="https://github.com/wzcai99/Pixel-Navigator">github</a>
              <p>
                We propose a pure RGB-based navigation skill, PixNav, which takes in an assigned pixel as goal specification and can be used to navigate towards any objects.
              </p>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/DiscussNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</span>
                <br>
                Yuxing Long, Xiaoqi Li, <strong>Wenzhe Cai</strong>, Hao Dong
                <br>
                <em>IEEE Conference on Robotics and Automation (ICRA), 2024</em>
                <br>
                <a href="https://sites.google.com/view/discussnav">website</a>
                /
                <a href="https://arxiv.org/pdf/2309.11382.pdf">paper</a>
                /
                <a href="https://github.com/LYX0501/DiscussNav">github</a>
                <p>
                  DiscussNav agent actively discusses with multiple domain experts before moving. And with multi-expert discussion, our method achieves zero-shot visual language navigation without any training.  
                </p>
              </td>
            </tr>
            </tbody></table>
          
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/AME-SAC.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Multi-Task Reinforcement Learning With Attention-Based Mixture of Experts</span>
                  <br>
                  Guangran Cheng, Lu Dong, <strong>Wenzhe Cai</strong>, Changyin Sun
                  <br>
                  <em>IEEE Robotics and Automation Letters (RA-L), 2023</em>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10111062">paper</a>
                  /
                  <a href="https://github.com/123penny123/AMESAC">github</a>
                  <p>
                  We propose a soft mixture of experts (MoE) based reinforcement learning method to tackle multi-task robotics control problems, which effectively captured the latent relationships among different tasks.
                  </p>
                </td>
              </tr>
              </tbody></table>

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/XuanPolicy.png' width="180">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <span class="papertitle">XuanCE: A Comprehensive and Unified Deep Reinforcement Learning Library</span>
                    <br>
                    Wenzhang Liu, <strong>Wenzhe Cai</strong>, Kun Jiang, Guangran Cheng, Yuanda Wang, Jiawei Wang, Jingyu Cao, Lele Xu, Chaoxu Mu, Changyin Sun
                    <br>
                    <em>Journel of Machine Learning Research (JMLR), 2023 (under review)</em>
                    <br>
                    <a href="https://github.com/agi-brain/xuanpolicy">github</a>
                    /
                    <a href="https://arxiv.org/pdf/2312.16248.pdf">paper</a>
                    <p>
                    XuanCE is an open-source ensemble of Deep Reinforcement Learning (DRL) algorithm implementations, which supports both single-agent RL and multi-agents RL algorithms.
                    </p>
                  </td>
                </tr>
                </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/DistillNav.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Robust Navigation with Cross-Modal Fusion and Knowledge Transfer</span>
                <br>
                <strong> Wenzhe Cai*</strong>, Guangran Cheng*, Lingyue Kong, Lu Dong, Changyin Sun 
                <br>
                <em>IEEE Conference on Robotics and Automation (ICRA), 2023</em>
                <br>
                <a href="https://sites.google.com/view/distillnav/">website</a>
                /
                <a href="https://arxiv.org/abs/2309.13266">paper</a>
                /
                <a href="https://github.com/wzcai99/Distill-Navigator">github</a>
                <p>
                  We propose a efficient distillation architecture to tackle the sim-to-real gap of an RL-based navigation policy. Our experiment shows our architecture outperforms the domain randomization techniques.
                </p>
              </td>
            </tr>
            </tbody></table>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/DGMem.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory</span>
                  <br>
                  <strong> Wenzhe Cai</strong>, Teng Wang, Guangran Cheng, Lele Xu, Changyin Sun 
                  <br>
                  <em>Applied Intelligence, 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2311.18473">paper</a>
                  <p>
                    We discuss the self-supervised navigation problem and present Dynamic Graph Memory (DGMem), which facilitates training only with on-board observations. 
                  </p>
                </td>
              </tr>
              </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/WorldModel.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Learning a World Model with Multi-Timescale Memory Augmentation</span>
                  <br>
                  <strong> Wenzhe Cai</strong>, Teng Wang, Jiawei Wang, Changyin Sun 
                  <br>
                  <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2022</em>
                  <br>
                  <a href="https://arxiv.org/abs/2309.13266">paper</a>
                  <p>
                  We propose a novel action-conditioned video prediction method which introduces optical flow prediction to model the influence of actions and incorporate the optical-flow based image prediction to improve the long-term prediction quality.
                  </p>
                </td>
              </tr>
              </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Services</h2>
                <p> Reviewer: RAL, TNNLS, TAI, ICRA, ICLR.</p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Honers & Awards</h2>
                <p> SEU Doctoral Entrance Scholarship (￥20000) <br>
                    SEU Scholarship from Shanghai Zhang Jiang Hi-Tech Park (￥10000) <br>
                    SEU Scholarship from Jiangsu Zhongnan Construction Group (￥10000) <br>
                    SEU Merit Student <br>
                    First Prize in Robocup Rescue Simulation Competition, China (1st) <br>
                    First Prize in Electronic Design Competition, JiangSu <br>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
